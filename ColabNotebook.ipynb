{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HleVyWVjmuDR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "import io\n",
        "import onnxruntime as ort\n",
        "# Set up ONNX Runtime session with DirectML\n",
        "from ultralytics import YOLO\n",
        "from scipy.spatial.distance import cdist\n",
        "import threading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm5xlNXZiSu2",
        "outputId": "cbf0ee2e-4fc2-49b4-ee21-2a9831955ae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.79-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m41.0/41.3 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m601.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime\n",
            "  Downloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.5-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Downloading ultralytics-8.2.79-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.1/869.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading ultralytics_thop-2.0.5-py3-none-any.whl (25 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, onnxruntime, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 onnxruntime-1.19.0 ultralytics-8.2.79 ultralytics-thop-2.0.5\n"
          ]
        }
      ],
      "source": [
        "pip install ultralytics onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "An3DaJKlnH38"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
        "from IPython.display import clear_output\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TrafficVisualization3D:\n",
        "    def __init__(self, map_size, road_network, car_model_path='benchi-ML500.json'):\n",
        "        self.map_size = map_size\n",
        "        self.road_network = road_network\n",
        "        self.fig = plt.figure(figsize=(8, 8), dpi=100)\n",
        "        self.ax = self.fig.add_subplot(111, projection='3d')\n",
        "        self.setup_plot()\n",
        "        self.load_car_model(car_model_path)\n",
        "\n",
        "    def load_car_model(self, car_model_path):\n",
        "        with open(car_model_path) as json_file:\n",
        "            data = json.load(json_file)\n",
        "            self.car_vertices = np.array(data['vertices'])\n",
        "            self.car_faces = np.array(data['faces']) - 1\n",
        "\n",
        "        # Scale up the car model\n",
        "        scale_factor = 8  # Increased scale factor\n",
        "        self.car_vertices *= scale_factor\n",
        "\n",
        "        # Rotate the car model to lie flat on the ground\n",
        "        rotation_matrix = np.array([\n",
        "            [1, 0, 0],\n",
        "            [0, 0, -1],\n",
        "            [0, 1, 0]\n",
        "        ])\n",
        "        self.car_vertices = np.dot(self.car_vertices, rotation_matrix)\n",
        "\n",
        "    def setup_plot(self):\n",
        "        self.ax.set_xlim(0, self.map_size[0])\n",
        "        self.ax.set_ylim(0, self.map_size[1])\n",
        "        self.ax.set_zlim(0, 50)  # Reduced z-limit for a better view\n",
        "        self.ax.axis('off')\n",
        "\n",
        "        # Set camera angle for a more immersive view\n",
        "        # Set camera angle for 3D perspective view\n",
        "        self.ax.view_init(elev=60, azim=80)\n",
        "        #Change the azimuth of the map.\n",
        "\n",
        "\n",
        "        for i in range(0, len(self.road_network), 4):\n",
        "            road = self.road_network[i:i+4]\n",
        "            x = road[:, 0]\n",
        "            y = road[:, 1]\n",
        "            z = np.zeros_like(x)\n",
        "            verts = [list(zip(x, y, z))]\n",
        "            self.ax.add_collection3d(Poly3DCollection(verts, facecolors='gray', alpha=0.5))\n",
        "\n",
        "        # Add ground plane\n",
        "        x = np.linspace(0, self.map_size[0], 2)\n",
        "        y = np.linspace(0, self.map_size[1], 2)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        Z = np.zeros_like(X)\n",
        "        self.ax.plot_surface(X, Y, Z, color='green', alpha=0.2)\n",
        "\n",
        "    def update(self, vehicles):\n",
        "        for collection in self.ax.collections:\n",
        "            collection.remove()\n",
        "\n",
        "        # Redraw road network and ground plane\n",
        "        self.setup_plot()\n",
        "\n",
        "        # Draw cars\n",
        "        for vehicle in vehicles[:10]:  # Increased to 10 cars\n",
        "            x, y = vehicle['position']\n",
        "            z = 0\n",
        "            self.draw_car(x, y, z, color='lightblue')\n",
        "\n",
        "        self.fig.canvas.draw()\n",
        "        img = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "        img = img.reshape(self.fig.canvas.get_width_height()[::-1] + (3,))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        return img\n",
        "\n",
        "    def draw_car(self, x, y, z, color='lightblue'):\n",
        "        # Translate the car model to the correct position\n",
        "        translated_vertices = self.car_vertices + np.array([x, y, z])\n",
        "\n",
        "        # Create a list of vertex coordinates for each face\n",
        "        mesh = Poly3DCollection([translated_vertices[face] for face in self.car_faces],\n",
        "                                facecolors=color, edgecolors='k', linewidths=0.1, alpha=0.8)\n",
        "        self.ax.add_collection3d(mesh)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9O3_zwtpn91O"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "from scipy.spatial.distance import cdist\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display, HTML\n",
        "from base64 import b64encode\n",
        "from google.colab import files\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class RunDetection2:\n",
        "    def __init__(self, video_path, model_path='yolov5su.onnx'):\n",
        "        self.video_path = video_path\n",
        "        self.map_size = (400, 400)\n",
        "        self.cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        plt.ion()  # Turn on interactive mode for real-time plotting\n",
        "\n",
        "        # Define the providers, if you're using CUDA replace AzureExecutionProvider... with CUDAExecutionProvider\n",
        "        providers = ['AzureExecutionProvider', 'CPUExecutionProvider']\n",
        "        self.session = ort.InferenceSession(model_path, providers=providers)\n",
        "\n",
        "        # Generate road network with two thick vertical lines\n",
        "        self.road_network = self.generate_vertical_roads()\n",
        "        self.visualizer = TrafficVisualization3D(self.map_size, self.road_network)\n",
        "\n",
        "        # Define source and destination points for homography\n",
        "        self.src_points = np.array([\n",
        "            [0, self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)],\n",
        "            [self.cap.get(cv2.CAP_PROP_FRAME_WIDTH), self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)],\n",
        "            [self.cap.get(cv2.CAP_PROP_FRAME_WIDTH), 0],\n",
        "            [0, 0]\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        self.dst_points = np.array([\n",
        "            [0, self.map_size[1]],\n",
        "            [self.map_size[0], self.map_size[1]],\n",
        "            [self.map_size[0], 0],\n",
        "            [0, 0]\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate homography matrix\n",
        "        self.H = cv2.getPerspectiveTransform(self.src_points, self.dst_points)\n",
        "\n",
        "        # Load car sprite\n",
        "        # self.car_sprite = cv2.imread(car_png, cv2.IMREAD_UNCHANGED)\n",
        "        # self.car_sprite = cv2.resize(self.car_sprite, (40, 80))  # Increased size\n",
        "\n",
        "        # Dictionary to store previous positions of cars\n",
        "        self.prev_positions = {}\n",
        "\n",
        "    def generate_vertical_roads(self):\n",
        "        road_width = 220  # Width of the vertical roads\n",
        "        center_x = self.map_size[0] // 2  # Center of the map\n",
        "        road_network = []  # List to store road network points\n",
        "        x1 = center_x - road_width // 2  # Start point of the first road\n",
        "        road_network.extend([(x1, 0), (x1, self.map_size[1]),\n",
        "         (x1 + road_width,\n",
        "          self.map_size[1]),\n",
        "           (x1 + road_width, 0)])\n",
        "        return np.array(road_network, dtype=np.int32)  # Convert to NumPy array\n",
        "\n",
        "\n",
        "    # def generate_vertical_roads(self):\n",
        "    #       road_width = 70  # Increased road width\n",
        "    #       road_spacing = 15 # Space between the two roads\n",
        "    #       road_network = []\n",
        "\n",
        "    #       # Calculate the center of the map\n",
        "    #       center_x = self.map_size[0] // 2\n",
        "\n",
        "    #     #\n",
        "\n",
        "\n",
        "    #       # Left vertical road\n",
        "    #       x1 = center_x - road_width - road_spacing // 2\n",
        "    #       road_network.extend([\n",
        "    #           [x1, 0],\n",
        "    #           [x1, self.map_size[1]],\n",
        "    #           [x1 + road_width, self.map_size[1]],\n",
        "    #           [x1 + road_width, 0]\n",
        "    #       ])\n",
        "\n",
        "    #       # Right vertical road\n",
        "    #       x2 = center_x + road_spacing // 2\n",
        "    #       road_network.extend([\n",
        "    #           [x2, 0],\n",
        "    #           [x2, self.map_size[1]],\n",
        "    #           [x2 + road_width, self.map_size[1]],\n",
        "    #           [x2 + road_width, 0]\n",
        "    #       ])\n",
        "\n",
        "    #       return np.array(road_network, dtype=np.int32)  # Convert to NumPy array\n",
        "\n",
        "\n",
        "    def process_output(self, output, img_size, conf_threshold=0.15, iou_threshold=0.35):\n",
        "        outputs = np.transpose(output[0])\n",
        "        rows = outputs.shape[0]\n",
        "        boxes = []\n",
        "        scores = []\n",
        "        class_ids = []\n",
        "\n",
        "        img_height, img_width = img_size\n",
        "        x_factor = img_width / 640\n",
        "        y_factor = img_height / 640\n",
        "\n",
        "        for i in range(rows):\n",
        "            classes_scores = outputs[i][4:]\n",
        "            max_score = np.amax(classes_scores)\n",
        "            if max_score >= conf_threshold:\n",
        "                class_id = np.argmax(classes_scores)\n",
        "                x, y, w, h = outputs[i][0], outputs[i][1], outputs[i][2], outputs[i][3]\n",
        "                left = int((x - w/2) * x_factor)\n",
        "                top = int((y - h/2) * y_factor)\n",
        "                width = int(w * x_factor)\n",
        "                height = int(h * y_factor)\n",
        "                boxes.append([left, top, width, height])\n",
        "                scores.append(max_score)\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        indices = cv2.dnn.NMSBoxes(boxes, scores, conf_threshold, iou_threshold)\n",
        "\n",
        "        detections = []\n",
        "        for i in indices:\n",
        "            box = boxes[i]\n",
        "            score = scores[i]\n",
        "            class_id = class_ids[i]\n",
        "            detections.append([box[0], box[1], box[0]+box[2], box[1]+box[3], score, class_id])\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def project_to_map(self, point):\n",
        "        px, py = point\n",
        "        projected_point = np.dot(self.H, np.array([px, py, 1]))\n",
        "        projected_point = projected_point / projected_point[2]\n",
        "        return (int(projected_point[0]), int(projected_point[1]))\n",
        "\n",
        "    def snap_to_road(self, point):\n",
        "        distances = cdist([point], self.road_network)\n",
        "        closest_segment_idx = np.argmin(distances)\n",
        "        if closest_segment_idx % 2 == 0:\n",
        "            start, end = self.road_network[closest_segment_idx], self.road_network[closest_segment_idx + 1]\n",
        "        else:\n",
        "            start, end = self.road_network[closest_segment_idx - 1], self.road_network[closest_segment_idx]\n",
        "\n",
        "        line_vec = end - start\n",
        "        point_vec = np.array(point) - start\n",
        "        projection = np.dot(point_vec, line_vec) / np.dot(line_vec, line_vec)\n",
        "        projection = np.clip(projection, 0, 1)\n",
        "\n",
        "        snapped_point = start + projection * line_vec\n",
        "\n",
        "        smoothing_factor = 0.6  # Adjust this value to control the smoothing effect\n",
        "        smooth_point = tuple(map(int, np.array(point) * (1 - smoothing_factor) + np.array(snapped_point) * smoothing_factor))\n",
        "\n",
        "        return smooth_point\n",
        "\n",
        "    def get_direction(self, current_pos, prev_pos):\n",
        "        if prev_pos is None:\n",
        "            return 0\n",
        "\n",
        "        dx = current_pos[0] - prev_pos[0]\n",
        "        dy = current_pos[1] - prev_pos[1]\n",
        "        angle = np.degrees(np.arctan2(dy, dx))\n",
        "        return angle\n",
        "\n",
        "    def rotate_sprite(self, sprite, angle):\n",
        "        rows, cols = sprite.shape[:2]\n",
        "        M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)\n",
        "        return cv2.warpAffine(sprite, M, (cols, rows), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_TRANSPARENT)\n",
        "\n",
        "\n",
        "    #Use tqdm to process the videos\n",
        "    def run(self, slowdown_factor=2):\n",
        "        total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frame_skip = 3  # Process every 4th frame\n",
        "        max_cars = 10  # Maximum number of cars to display\n",
        "        frame_count = 0\n",
        "        print(\"Processing the video\")\n",
        "\n",
        "        # Get video properties\n",
        "        width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = int(self.cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "        # Create output video file with reduced fps\n",
        "        output_filename = 'output_video_slow3.mp4'\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_filename, fourcc, fps // slowdown_factor, (width, height))\n",
        "        with tqdm(total=total_frames // frame_skip, desc=\"Processing Frames\") as pbar:\n",
        "          while self.cap.isOpened():\n",
        "            ret, frame = self.cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_count += 1\n",
        "            if frame_count % frame_skip != 0:\n",
        "                continue\n",
        "\n",
        "            # Preprocess the frame\n",
        "            input_height, input_width = 640, 640 # Resize to 1280,1280\n",
        "            img = cv2.resize(frame, (input_width, input_height))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = img.transpose((2, 0, 1)).astype(np.float32)\n",
        "            img /= 255.0\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "\n",
        "            # Run inference\n",
        "            inputs = {self.session.get_inputs()[0].name: img}\n",
        "            outputs = self.session.run(None, inputs)\n",
        "\n",
        "            # Process the output\n",
        "            detections = self.process_output(outputs[0], frame.shape[:2])\n",
        "\n",
        "            frame_vehicles = []\n",
        "            for detection in detections[:max_cars]:\n",
        "                x1, y1, x2, y2, score, class_id = detection\n",
        "                if class_id == 2:  # Assuming class ID 2 is for cars, adjust if needed\n",
        "                    car_position = ((x1 + x2) // 2, y2)  # Bottom center of bounding box\n",
        "                    map_position = self.project_to_map(car_position)\n",
        "                    snapped_position = self.snap_to_road(map_position)\n",
        "                    frame_vehicles.append({\n",
        "                        'position': snapped_position,\n",
        "                        'type': 'car',\n",
        "                        'score': score\n",
        "                    })\n",
        "\n",
        "            # Update the visualization\n",
        "            map_view = self.visualizer.update(frame_vehicles)\n",
        "\n",
        "            # Resize the map view to fit in the corner\n",
        "            map_view = cv2.resize(map_view, (frame.shape[1] // 3, frame.shape[0] // 3))\n",
        "\n",
        "            # Create a region of interest (ROI) in the top left corner of each frame\n",
        "            roi = frame[0:map_view.shape[0], 0:map_view.shape[1]]\n",
        "\n",
        "            # Create a mask of the map view and its inverse mask\n",
        "            map_gray = cv2.cvtColor(map_view, cv2.COLOR_BGR2GRAY)\n",
        "            _, mask = cv2.threshold(map_gray, 1, 255, cv2.THRESH_BINARY)\n",
        "            mask_inv = cv2.bitwise_not(mask)\n",
        "\n",
        "            # Black-out the area of the map view in ROI\n",
        "            roi_bg = cv2.bitwise_and(roi, roi, mask=mask_inv)\n",
        "\n",
        "            # Take only region of map view from map image\n",
        "            map_fg = cv2.bitwise_and(map_view, map_view, mask=mask)\n",
        "\n",
        "            # Put map view in ROI and modify the main image\n",
        "            dst = cv2.add(roi_bg, map_fg)\n",
        "            frame[0:map_view.shape[0], 0:map_view.shape[1]] = dst\n",
        "\n",
        "            # Write the frame to the output video multiple times to create slow-motion effect\n",
        "            for _ in range(slowdown_factor):\n",
        "                out.write(frame)\n",
        "            pbar.update(1)\n",
        "\n",
        "\n",
        "            # Optional: Print progress\n",
        "            if frame_count % 100 == 0:\n",
        "                print(f\"Processed {frame_count} frames\")\n",
        "\n",
        "\n",
        "        # Release resources\n",
        "        self.cap.release()\n",
        "        out.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "        print(f\"Video processing complete. Slow-motion output saved as {output_filename}\")\n",
        "        return output_filename\n",
        "\n",
        "    def process_video(self, slowdown_factor=2):\n",
        "        output_filename = self.run(slowdown_factor)\n",
        "\n",
        "        # Provide download link\n",
        "        files.download(output_filename)\n",
        "\n",
        "        # Display a message with instructions\n",
        "        display(HTML(f\"\"\"\n",
        "        <p>Video processing complete. The slow-motion output video has been saved as {output_filename}.</p>\n",
        "        <p>If the download doesn't start automatically, please check your browser's download manager.</p>\n",
        "        <p>You can also find the video file in the Colab file browser (folder icon in the left sidebar).</p>\n",
        "        \"\"\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6hQ0z3VTmc4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "af0eab45c8134e69b17992d8ce0f7c49",
            "dc2393d5055d4b4b862c1f5c2d0db3aa",
            "118bb1a1147e415c8d0f53b5c20ac977",
            "e6aa8e7163014476bdb234b707fc807d",
            "1331234599e84a5f919ea2dea0a892d4",
            "544eeb3ce7334bac8415c0df801361d7",
            "f0c2243055644e1ab9a53b4ccba322dc",
            "dfe22c512296488ab8b62e954ea7b7b4",
            "c1511f61b91a4c18b1255c5a7c0a1820",
            "2f499f2c01c440da892be6c1cb1f28ee",
            "a039b82297474825a1df8eafec2f0565"
          ]
        },
        "id": "-AON_K25oG7D",
        "outputId": "170c7dd7-bc55-446f-c14f-f551e3fa01af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing the video\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af0eab45c8134e69b17992d8ce0f7c49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Frames:   0%|          | 0/111 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Usage\n",
        "#video\n",
        "video = 'newvid4.mp4'\n",
        "#model\n",
        "model = 'yolov8l.onnx'\n",
        "#detection\n",
        "detector = RunDetection2(video,model)\n",
        "#slow down the video\n",
        "detector.process_video(slowdown_factor=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "ashHGMY74coA",
        "outputId": "cd02e209-aaf4-423a-9aaf-3c3dcf4e44a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l.pt to 'yolov8l.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 83.7M/83.7M [00:00<00:00, 93.5MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.79 🚀 Python-3.10.12 torch-2.3.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8l summary (fused): 268 layers, 43,668,288 parameters, 0 gradients, 165.2 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8l.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (83.7 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0'] not found, attempting AutoUpdate...\n",
            "Collecting onnx>=1.12.0\n",
            "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (3.20.3)\n",
            "Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.9/15.9 MB 196.7 MB/s eta 0:00:00\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.16.2\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 7.7s, installed 1 package: ['onnx>=1.12.0']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.2 opset 17...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 19.5s, saved as 'yolov8l.onnx' (166.8 MB)\n",
            "\n",
            "Export complete (31.3s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolov8l.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolov8l.onnx imgsz=640 data=coco.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'yolov8l.onnx'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO('yolov8l.pt')\n",
        "#Model yolov5xu\n",
        "#or model YOLOv8l\n",
        "model.export(format='onnx')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "118bb1a1147e415c8d0f53b5c20ac977": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfe22c512296488ab8b62e954ea7b7b4",
            "max": 111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1511f61b91a4c18b1255c5a7c0a1820",
            "value": 1
          }
        },
        "1331234599e84a5f919ea2dea0a892d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f499f2c01c440da892be6c1cb1f28ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "544eeb3ce7334bac8415c0df801361d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a039b82297474825a1df8eafec2f0565": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af0eab45c8134e69b17992d8ce0f7c49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc2393d5055d4b4b862c1f5c2d0db3aa",
              "IPY_MODEL_118bb1a1147e415c8d0f53b5c20ac977",
              "IPY_MODEL_e6aa8e7163014476bdb234b707fc807d"
            ],
            "layout": "IPY_MODEL_1331234599e84a5f919ea2dea0a892d4"
          }
        },
        "c1511f61b91a4c18b1255c5a7c0a1820": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc2393d5055d4b4b862c1f5c2d0db3aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_544eeb3ce7334bac8415c0df801361d7",
            "placeholder": "​",
            "style": "IPY_MODEL_f0c2243055644e1ab9a53b4ccba322dc",
            "value": "Processing Frames:   1%"
          }
        },
        "dfe22c512296488ab8b62e954ea7b7b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6aa8e7163014476bdb234b707fc807d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f499f2c01c440da892be6c1cb1f28ee",
            "placeholder": "​",
            "style": "IPY_MODEL_a039b82297474825a1df8eafec2f0565",
            "value": " 1/111 [00:04&lt;08:28,  4.62s/it]"
          }
        },
        "f0c2243055644e1ab9a53b4ccba322dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
